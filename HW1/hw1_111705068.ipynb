{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b564d7",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ba5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import html\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "_whitespace_re = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "_word_re = re.compile(r\"[A-Za-z_][A-Za-z0-9_]*\")\n",
    "# Punctuation/delimiters typical for C/Java/JS/Solidity/Python\n",
    "# We'll split code tokens by placing spaces around these, then split.\n",
    "_separators = r\"[\\(\\)\\[\\]\\{\\};:,\\.\\+\\-\\*/%&\\|\\^!=<>\\?~]\"\n",
    "_sep_re = re.compile(f\"({_separators})\")\n",
    "\n",
    "# String literal patterns (\"...\", '...', `...`)\n",
    "_str_re = re.compile(r'(\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\'|\".*?\"|\\'.*?\\'|`.*?`)', re.DOTALL)\n",
    "_num_re = re.compile(r\"\\b\\d+(?:_\\d+)*(?:\\.[0-9_]+)?\\b\")\n",
    "\n",
    "# Comments (C-like): // line, /* block */\n",
    "_c_line_cmt = re.compile(r\"//.*?(?=\\n|$)\")\n",
    "_c_block_cmt = re.compile(r\"/\\*.*?\\*/\", re.DOTALL)\n",
    "# Python: # line, triple-quoted docstrings (we'll treat as comments)\n",
    "_py_line_cmt = re.compile(r\"#.*?(?=\\n|$)\")\n",
    "_py_doc_cmt = re.compile(r'(\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\')', re.DOTALL)\n",
    "\n",
    "_c_like_file_ext = {\".c\", \".h\", \".cpp\", \".hpp\", \".cc\", \".java\", \".js\", \".ts\", \".sol\", \".cs\", \".swift\", \".go\"}\n",
    "_py_like_file_ext = {\".py\"}\n",
    "\n",
    "@dataclass\n",
    "class PreprocessConfig:\n",
    "    lowercase: bool = True\n",
    "    normalize_numbers: bool = True   # replace numbers with <NUM>\n",
    "    normalize_strings: bool = True   # replace strings with <STR>\n",
    "    split_identifiers: bool = True\n",
    "    keep_empty: bool = False\n",
    "\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    return _whitespace_re.sub(\" \", text).strip()\n",
    "\n",
    "\n",
    "def split_identifiers(token: str) -> List[str]:\n",
    "    \"\"\"Split camelCase/mixedCase and snake_case identifiers.\n",
    "    e.g. `allowedAmount_total` -> [\"allowed\", \"Amount\", \"total\"] -> lowercased later\n",
    "    \"\"\"\n",
    "    # First split snake_case\n",
    "    parts = re.split(r\"_+\", token)\n",
    "    out: List[str] = []\n",
    "    for p in parts:\n",
    "        # Split on camelCase transitions (including digits)\n",
    "        # e.g. HTTPServerError -> HTTP, Server, Error\n",
    "        for m in re.finditer(r\"[A-Z]?[a-z]+|[A-Z]+(?![a-z])|\\d+\", p):\n",
    "            out.append(m.group(0))\n",
    "    return [t for t in out if t]\n",
    "\n",
    "\n",
    "def extract_comments_and_code(text: str, language_hint: str = \"auto\") -> Tuple[str, str]:\n",
    "    \"\"\"Return (comments_text, code_without_comments).\n",
    "    Supports a broad regex-based pass for C-like and Python-like languages.\n",
    "    \"\"\"\n",
    "    s = text\n",
    "    comments: List[str] = []\n",
    "\n",
    "    # Python-like docstrings first to avoid gobbling by string regex\n",
    "    for m in _py_doc_cmt.finditer(s):\n",
    "        comments.append(m.group(0))\n",
    "    s = _py_doc_cmt.sub(\"\\n\", s)\n",
    "\n",
    "    # C-like block comments\n",
    "    for m in _c_block_cmt.finditer(s):\n",
    "        comments.append(m.group(0))\n",
    "    s = _c_block_cmt.sub(\"\\n\", s)\n",
    "\n",
    "    # Line comments\n",
    "    for m in _c_line_cmt.finditer(s):\n",
    "        comments.append(m.group(0))\n",
    "    s = _c_line_cmt.sub(\"\\n\", s)\n",
    "\n",
    "    for m in _py_line_cmt.finditer(s):\n",
    "        comments.append(m.group(0))\n",
    "    s = _py_line_cmt.sub(\"\\n\", s)\n",
    "\n",
    "    comments_text = \"\\n\".join(comments)\n",
    "    code_wo_comments = s\n",
    "    return comments_text, code_wo_comments\n",
    "\n",
    "\n",
    "def tokenize_comment_text(text: str, cfg: PreprocessConfig) -> List[str]:\n",
    "    text = html.unescape(text)\n",
    "    if cfg.normalize_strings:\n",
    "        text = _str_re.sub(\" <STR> \", text)\n",
    "    if cfg.normalize_numbers:\n",
    "        text = _num_re.sub(\" <NUM> \", text)\n",
    "    # Words only for comments; punctuation is less useful\n",
    "    toks = _word_re.findall(text)\n",
    "    if cfg.split_identifiers:\n",
    "        expanded: List[str] = []\n",
    "        for tok in toks:\n",
    "            expanded.extend(split_identifiers(tok))\n",
    "        toks = expanded\n",
    "    if cfg.lowercase:\n",
    "        toks = [t.lower() for t in toks]\n",
    "    return toks if toks or cfg.keep_empty else (toks or [])\n",
    "\n",
    "\n",
    "def tokenize_code(text: str, cfg: PreprocessConfig) -> List[str]:\n",
    "    text = html.unescape(text)\n",
    "    # Replace string and numbers with placeholders to reduce noise\n",
    "    if cfg.normalize_strings:\n",
    "        text = _str_re.sub(\" <STR> \", text)\n",
    "    if cfg.normalize_numbers:\n",
    "        text = _num_re.sub(\" <NUM> \", text)\n",
    "    # Put spaces around separators so we can split and KEEP them\n",
    "    text = _sep_re.sub(r\" \\1 \", text)\n",
    "    # Collapse whitespace\n",
    "    text = normalize_whitespace(text)\n",
    "    raw_toks = text.split(\" \") if text else []\n",
    "\n",
    "    toks: List[str] = []\n",
    "    for tok in raw_toks:\n",
    "        if not tok:\n",
    "            continue\n",
    "        if tok == \"<STR>\" or tok == \"<NUM>\" or _sep_re.fullmatch(tok):\n",
    "            toks.append(tok)\n",
    "        else:\n",
    "            if cfg.split_identifiers:\n",
    "                toks.extend(split_identifiers(tok))\n",
    "            else:\n",
    "                toks.append(tok)\n",
    "    if cfg.lowercase:\n",
    "        toks = [t.lower() for t in toks]\n",
    "    return toks if toks or cfg.keep_empty else (toks or [])\n",
    "\n",
    "\n",
    "def preprocess_code_snippet(snippet: str, cfg: PreprocessConfig) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Given raw code snippet, return (comment_tokens, code_tokens).\"\"\"\n",
    "    comments_text, code_wo = extract_comments_and_code(snippet)\n",
    "    cmt_toks = tokenize_comment_text(comments_text, cfg)\n",
    "    code_toks = tokenize_code(code_wo, cfg)\n",
    "    return cmt_toks, code_toks\n",
    "\n",
    "\n",
    "def preprocess_query(q: str, cfg: PreprocessConfig) -> List[str]:\n",
    "    # For queries, treat like comment text (natural language)\n",
    "    return tokenize_comment_text(q, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31b9237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_csv = \"data/code_snippets.csv\"\n",
    "queries_csv = \"data/test_queries.csv\"\n",
    "out_codes_csv = \"data/code_snippets_proc.csv\"\n",
    "out_queries_csv = \"data/test_queries_proc.csv\"\n",
    "\n",
    "cfg = PreprocessConfig(\n",
    "        lowercase=True,\n",
    "        split_identifiers=True,\n",
    "        normalize_numbers=True,\n",
    "        normalize_strings=True,\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(codes_csv, engine=\"python\")\n",
    "if \"code\" not in df.columns:\n",
    "    raise ValueError(\"codes CSV must contain a 'code' column\")\n",
    "comment_tokens_list: List[List[str]] = []\n",
    "code_tokens_list: List[List[str]] = []\n",
    "all_tokens_list: List[List[str]] = []\n",
    "\n",
    "for s in df[\"code\"].astype(str).tolist():\n",
    "    cmt_toks, code_toks = preprocess_code_snippet(s, cfg)\n",
    "    comment_tokens_list.append(cmt_toks)\n",
    "    code_tokens_list.append(code_toks)\n",
    "    all_tokens_list.append(cmt_toks + code_toks)\n",
    "\n",
    "# df[\"comment_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in comment_tokens_list]\n",
    "df[\"code_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in code_tokens_list]\n",
    "# df[\"all_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in all_tokens_list]\n",
    "df.to_csv(out_codes_csv, index=False)\n",
    "\n",
    "# Process queries\n",
    "df = pd.read_csv(queries_csv, engine=\"python\")\n",
    "if \"query\" not in df.columns:\n",
    "    raise ValueError(\"queries CSV must contain a 'query' column\")\n",
    "query_tokens_list: List[List[str]] = []\n",
    "for s in df[\"query\"].astype(str).tolist():\n",
    "    q_toks = preprocess_query(s, cfg)\n",
    "    query_tokens_list.append(q_toks)\n",
    "df[\"query_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in query_tokens_list]\n",
    "df.to_csv(out_queries_csv, index=False)\n",
    "\n",
    "code_snippets = pd.read_csv(out_codes_csv, engine=\"python\")['code_tokens']\n",
    "queries = pd.read_csv(out_queries_csv, engine=\"python\")['query_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_submission_file(results, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"query_id,code_id\\n\")\n",
    "        for query_id, code_ids in enumerate(results):\n",
    "            code_ids_str = ' '.join(map(str, code_ids))\n",
    "            f.write(f\"{query_id+1},{code_ids_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b47891",
   "metadata": {},
   "source": [
    "# Sparse Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba89168",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60295bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "class TFIDF:\n",
    "    '''\n",
    "    documents: List of documents (strings)\n",
    "    term_freqs: List of term frequency dictionaries for each document ex: [{'term1': 2, 'term2': 1}, ...]\n",
    "    doc_freqs: Document frequency dictionary for terms ex: {'term1': 3, 'term2': 5}\n",
    "    num_docs: Total number of documents\n",
    "    vocabulary: Set of unique terms across all documents\n",
    "    idf: Inverse Document Frequency dictionary for terms\n",
    "    tfidf_matrix: 2D numpy array storing TF-IDF scores for documents\n",
    "    vocab_to_idx: Mapping from term to its index in the vocabulary\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.term_freqs = []\n",
    "        self.doc_freqs = defaultdict(int)\n",
    "        self.num_docs = 0\n",
    "        self.vocabulary = set()\n",
    "        self.idf = defaultdict(float)\n",
    "        self.tfidf_matrix = None\n",
    "        self.vocab_to_idx = defaultdict(int)\n",
    "\n",
    "    def add_document(self, document_tokens):\n",
    "        if isinstance(document_tokens, str):\n",
    "            try:\n",
    "                tokens = json.loads(document_tokens)\n",
    "            except json.JSONDecodeError:\n",
    "                \n",
    "                tokens = document_tokens.split()\n",
    "        else:\n",
    "            tokens = document_tokens\n",
    "        self.documents.append(tokens)\n",
    "        self.num_docs += 1\n",
    "        term_count = defaultdict(int)\n",
    "\n",
    "        for term in tokens:\n",
    "            term_count[term] += 1\n",
    "            self.vocabulary.add(term)\n",
    "\n",
    "        self.term_freqs.append(term_count)\n",
    "\n",
    "        for term in term_count.keys():\n",
    "            self.doc_freqs[term] += 1\n",
    "    \n",
    "    def _build_vocab_index(self):\n",
    "        self.vocabulary = list(self.vocabulary)\n",
    "        self.vocab_to_idx = {term: idx for idx, term in enumerate(self.vocabulary)}\n",
    "        \n",
    "    def compute_tfidf(self):\n",
    "        if self.tfidf_matrix is not None:\n",
    "            return self.tfidf_matrix\n",
    "            \n",
    "        self._build_vocab_index()\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        for term in self.vocabulary:\n",
    "            self.idf[term] = math.log((1 + (self.num_docs/self.doc_freqs[term])), 2)\n",
    "        \n",
    "        self.tfidf_matrix = np.zeros((self.num_docs, vocab_size))\n",
    "        \n",
    "        for doc_idx, term_count in enumerate(self.term_freqs):\n",
    "            for term, count in term_count.items():\n",
    "                if term in self.vocab_to_idx:\n",
    "                    tf = 1 + math.log(count, 2)\n",
    "                    tfidf_val = tf * self.idf[term]\n",
    "                    self.tfidf_matrix[doc_idx][self.vocab_to_idx[term]] = tfidf_val\n",
    "        \n",
    "        return self.tfidf_matrix\n",
    "\n",
    "    def get_query_vector(self, query_tokens):\n",
    "        \n",
    "        if self.tfidf_matrix is None:\n",
    "            self.compute_tfidf()\n",
    "        \n",
    "        if isinstance(query_tokens, str):\n",
    "            try:\n",
    "                tokens = json.loads(query_tokens)\n",
    "            except json.JSONDecodeError:\n",
    "                tokens = query_tokens.split()\n",
    "        else:\n",
    "            tokens = query_tokens\n",
    "        \n",
    "        query_term_count = defaultdict(int)\n",
    "        for term in tokens:\n",
    "            query_term_count[term] += 1\n",
    "\n",
    "        query_vector = np.zeros(len(self.vocabulary))\n",
    "        \n",
    "        for term, count in query_term_count.items():\n",
    "            if term in self.vocab_to_idx:\n",
    "                tf = 1 + math.log(count, 2)\n",
    "                idf = self.idf.get(term, 0.0)\n",
    "                query_vector[self.vocab_to_idx[term]] = tf * idf\n",
    "                \n",
    "        return query_vector\n",
    "    \n",
    "    def compute_similarity_batch(self, query_vector):\n",
    "        \n",
    "        dot_products = np.dot(self.tfidf_matrix, query_vector)\n",
    "        \n",
    "        # Document Length Normalization\n",
    "        doc_norms = np.linalg.norm(self.tfidf_matrix, axis=1)\n",
    "        query_norm = np.linalg.norm(query_vector)\n",
    "        \n",
    "        valid_mask = (doc_norms > 0) & (query_norm > 0)\n",
    "        similarities = np.zeros(len(doc_norms))\n",
    "        similarities[valid_mask] = dot_products[valid_mask] / (doc_norms[valid_mask] * query_norm)\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def get_top_k_similar_documents(self, query_tokens, k):\n",
    "        \n",
    "        query_vector = self.get_query_vector(query_tokens)\n",
    "        similarities = self.compute_similarity_batch(query_vector)\n",
    "        \n",
    "        if k >= len(similarities):\n",
    "            top_k_indices = np.argsort(similarities)[::-1]\n",
    "        else:\n",
    "            top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
    "            top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
    "        \n",
    "        # need to plus 1 to indices for 1-based indexing\n",
    "        top_k_indices += 1\n",
    "        return top_k_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "805e255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TFIDF()\n",
    "\n",
    "for code in code_snippets:\n",
    "    tfidf.add_document(code)\n",
    "    \n",
    "tfidf.compute_tfidf()\n",
    "\n",
    "top_k_results = []\n",
    "for query in queries:\n",
    "    top_k = tfidf.get_top_k_similar_documents(query, 10)\n",
    "    top_k_results.append(top_k)\n",
    "\n",
    "output_submission_file(top_k_results, 'results/tf_idf_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ca4ab",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e32fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 implementation\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        self.documents = []\n",
    "        self.term_freqs = []\n",
    "        self.doc_freqs = defaultdict(int)\n",
    "        self.num_docs = 0\n",
    "        self.vocabulary = set()\n",
    "        self.doc_lengths = []\n",
    "        self.avg_doc_len = 0\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self._precomputed_idf = {}\n",
    "        self._precomputed_norms = None\n",
    "\n",
    "    def add_document(self, document_tokens):\n",
    "        if isinstance(document_tokens, str):\n",
    "            try:\n",
    "                terms = json.loads(document_tokens)\n",
    "            except json.JSONDecodeError:\n",
    "                terms = document_tokens.split()\n",
    "        else:\n",
    "            terms = document_tokens\n",
    "            \n",
    "        self.documents.append(terms)\n",
    "        self.doc_lengths.append(len(terms))\n",
    "        self.num_docs += 1\n",
    "        term_count = defaultdict(int)\n",
    "\n",
    "        for term in terms:\n",
    "            term_count[term] += 1\n",
    "            self.vocabulary.add(term)\n",
    "\n",
    "        self.term_freqs.append(term_count)\n",
    "\n",
    "        for term in term_count.keys():\n",
    "            self.doc_freqs[term] += 1\n",
    "    \n",
    "    def _precompute_idf(self):\n",
    "        self.avg_doc_len = sum(self.doc_lengths) / self.num_docs\n",
    "        for term in self.vocabulary:\n",
    "            df = self.doc_freqs[term]\n",
    "            self._precomputed_idf[term] = math.log((self.num_docs - df + 0.5) / (df + 0.5))\n",
    "    \n",
    "    def _precompute_normalization_factors(self):\n",
    "        self._precomputed_norms = np.array([\n",
    "            self.k1 * (1 - self.b + self.b * (doc_len / self.avg_doc_len))\n",
    "            for doc_len in self.doc_lengths\n",
    "        ])\n",
    "    \n",
    "    def compute_similarity_batch(self, query_tokens):\n",
    "        if not self._precomputed_idf:\n",
    "            self._precompute_idf()\n",
    "        if self._precomputed_norms is None:\n",
    "            self._precompute_normalization_factors()\n",
    "            \n",
    "        if isinstance(query_tokens, str):\n",
    "            try:\n",
    "                query_terms = json.loads(query_tokens)\n",
    "            except json.JSONDecodeError:\n",
    "                query_terms = query_tokens.split()\n",
    "        else:\n",
    "            query_terms = query_tokens\n",
    "            \n",
    "        similarities = np.zeros(self.num_docs)\n",
    "        \n",
    "        for doc_idx in range(self.num_docs):\n",
    "            score = 0.0\n",
    "            term_freq_doc = self.term_freqs[doc_idx]\n",
    "            norm_factor = self._precomputed_norms[doc_idx]\n",
    "            \n",
    "            for term in query_terms:\n",
    "                if term in self.vocabulary:\n",
    "                    tf = term_freq_doc.get(term, 0)\n",
    "                    if tf > 0:  # 只計算有出現的詞彙\n",
    "                        idf = self._precomputed_idf[term]\n",
    "                        normalized_tf = (tf * (self.k1 + 1)) / (tf + norm_factor)\n",
    "                        score += idf * normalized_tf\n",
    "            \n",
    "            similarities[doc_idx] = score\n",
    "\n",
    "        return similarities\n",
    "\n",
    "    def get_top_k_similar_documents(self, query_tokens, k):\n",
    "        similarities = self.compute_similarity_batch(query_tokens)\n",
    "\n",
    "        if k >= len(similarities):\n",
    "            top_k_indices = np.argsort(similarities)[::-1]      \n",
    "        else:\n",
    "            top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
    "            top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
    "        \n",
    "        # need to plus 1 to indices for 1-based indexing\n",
    "        top_k_indices += 1\n",
    "        return top_k_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb9dce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25()\n",
    "\n",
    "for code in code_snippets:\n",
    "    bm25.add_document(code)\n",
    "\n",
    "top_k_results = []\n",
    "for query in queries:\n",
    "    top_k = bm25.get_top_k_similar_documents(query, 10)\n",
    "    top_k_results.append(top_k)\n",
    "\n",
    "output_submission_file(top_k_results, 'results/bm25_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42323e28",
   "metadata": {},
   "source": [
    "# Dense Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a17d32",
   "metadata": {},
   "source": [
    "## Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "803c2c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:21<00:00,  1.48it/s]\n",
      "100%|██████████| 32/32 [00:28<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def format_str(string):\n",
    "    for char in ['\\r\\n', '\\r', '\\n']:\n",
    "        string = string.replace(char, ' ')\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "code_snippets = pd.read_csv(out_codes_csv, engine=\"python\")['code']\n",
    "queries = pd.read_csv(out_queries_csv, engine=\"python\")['query']\n",
    "\n",
    "codes = [format_str(c)[:512] for c in code_snippets]\n",
    "queries = [format_str(q)[:512] for q in queries]\n",
    "\n",
    "def get_embeddings(text_list, batch_size=16):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(text_list), batch_size)):\n",
    "        batch_texts = text_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]   # 取 [CLS] 向量\n",
    "            norm_embeddings = F.normalize(cls_embeddings, p=2, dim=1)\n",
    "            all_embeddings.append(norm_embeddings)\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "query_embs = get_embeddings(queries)\n",
    "code_embs  = get_embeddings(codes)\n",
    "\n",
    "sim_matrix = torch.matmul(query_embs, code_embs.T)\n",
    "\n",
    "top_k = 10\n",
    "results = []\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    top_indices = torch.topk(sim_matrix[i], k=top_k).indices\n",
    "    results.append(top_indices.cpu().numpy().tolist())\n",
    "    \n",
    "output_submission_file(results, 'results/pre_trained_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853eb16e",
   "metadata": {},
   "source": [
    "### Origin tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8862d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:27<00:00,  1.18it/s]\n",
      "100%|██████████| 32/32 [00:29<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 = 0.1440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.eval()\n",
    "\n",
    "def format_str(string):\n",
    "    for char in ['\\r\\n', '\\r', '\\n']:\n",
    "        string = string.replace(char, ' ')\n",
    "    return string.strip()\n",
    "\n",
    "df = pd.read_csv(\"data/train_queries.csv\", engine=\"python\")\n",
    "codes = [format_str(c)[:512] for c in df[\"code\"]]\n",
    "queries = [format_str(q)[:512] for q in df[\"query\"]]\n",
    "\n",
    "\n",
    "def get_embeddings(text_list, batch_size=16):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(text_list), batch_size)):\n",
    "        batch_texts = text_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding='max_length', truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            norm_embeddings = F.normalize(cls_embeddings, p=2, dim=1)\n",
    "            all_embeddings.append(norm_embeddings)\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "query_embs = get_embeddings(queries)\n",
    "code_embs  = get_embeddings(codes)\n",
    "\n",
    "sim_matrix = torch.matmul(query_embs, code_embs.T)\n",
    "\n",
    "\n",
    "def recall_at_k(sim_matrix, k=10):\n",
    "    N = sim_matrix.size(0)\n",
    "    _, topk_idx = torch.topk(sim_matrix, k, dim=1) \n",
    "    correct = 0\n",
    "    for i in range(N):\n",
    "        if i in topk_idx[i]: \n",
    "            correct += 1\n",
    "    return correct / N\n",
    "\n",
    "recall10 = recall_at_k(sim_matrix, k=10)\n",
    "print(f\"Recall@10 = {recall10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353aff3",
   "metadata": {},
   "source": [
    "### Self-define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c501ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = \"data/train_queries.csv\"\n",
    "output_csv = \"data/train_queries_proc.csv\"\n",
    "\n",
    "cfg = PreprocessConfig(\n",
    "        lowercase=True,\n",
    "        split_identifiers=True,\n",
    "        normalize_numbers=True,\n",
    "        normalize_strings=True,\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(input_csv, engine=\"python\")\n",
    "if \"code\" not in df.columns:\n",
    "    raise ValueError(\"codes CSV must contain a 'code' column\")\n",
    "comment_tokens_list: List[List[str]] = []\n",
    "code_tokens_list: List[List[str]] = []\n",
    "all_tokens_list: List[List[str]] = []\n",
    "\n",
    "for s in df[\"code\"].astype(str).tolist():\n",
    "    cmt_toks, code_toks = preprocess_code_snippet(s, cfg)\n",
    "    comment_tokens_list.append(cmt_toks)\n",
    "    code_tokens_list.append(code_toks)\n",
    "    all_tokens_list.append(cmt_toks + code_toks)\n",
    "\n",
    "df[\"code_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in code_tokens_list]\n",
    "\n",
    "df2 = pd.read_csv(input_csv, engine=\"python\")\n",
    "if \"query\" not in df2.columns:\n",
    "    raise ValueError(\"queries CSV must contain a 'query' column\")\n",
    "query_tokens_list: List[List[str]] = []\n",
    "for s in df2[\"query\"].astype(str).tolist():\n",
    "    q_toks = preprocess_query(s, cfg)\n",
    "    query_tokens_list.append(q_toks)\n",
    "df2[\"query_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in query_tokens_list]\n",
    "\n",
    "df_final = pd.concat([df[\"code_tokens\"], df2[\"query_tokens\"]], axis=1)\n",
    "df_final.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b73a2dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:18<00:00,  1.75it/s]\n",
      "100%|██████████| 32/32 [00:46<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 = 0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "model.eval()\n",
    "\n",
    "def format_str(string):\n",
    "    for char in ['\\r\\n', '\\r', '\\n']:\n",
    "        string = string.replace(char, ' ')\n",
    "    return string.strip()\n",
    "\n",
    "df = pd.read_csv(\"data/train_queries_proc.csv\", engine=\"python\")\n",
    "\n",
    "codes = [eval(c) for c in df[\"code_tokens\"]]      \n",
    "queries = [eval(q) for q in df[\"query_tokens\"]]\n",
    "\n",
    "def tokens_to_ids_batch(token_lists, pad_token_id, max_len=256):\n",
    "    id_lists = [tokenizer.convert_tokens_to_ids(tokens[:max_len]) for tokens in token_lists]\n",
    "    max_len_in_batch = min(max_len, max(len(ids) for ids in id_lists))\n",
    "    padded = [ids + [pad_token_id] * (max_len_in_batch - len(ids)) for ids in id_lists]\n",
    "    attention_masks = [[1] * len(ids) + [0] * (max_len_in_batch - len(ids)) for ids in id_lists]\n",
    "    return torch.tensor(padded), torch.tensor(attention_masks)\n",
    "\n",
    "def get_embeddings_from_tokens(token_lists, batch_size=16, max_len=512):\n",
    "    all_embeddings = []\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id \n",
    "    for i in tqdm(range(0, len(token_lists), batch_size)):\n",
    "        batch_tokens = token_lists[i:i+batch_size]\n",
    "        input_ids, attn_masks = tokens_to_ids_batch(batch_tokens, pad_id, max_len)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_masks)\n",
    "            cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "            norm_emb = F.normalize(cls_emb, p=2, dim=1)\n",
    "            all_embeddings.append(norm_emb)\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "query_embs = get_embeddings_from_tokens(queries)\n",
    "code_embs  = get_embeddings_from_tokens(codes)\n",
    "\n",
    "sim_matrix = torch.matmul(query_embs, code_embs.T)\n",
    "\n",
    "def recall_at_k(sim_matrix, k=10):\n",
    "    N = sim_matrix.size(0)\n",
    "    _, topk_idx = torch.topk(sim_matrix, k, dim=1)\n",
    "    correct = 0\n",
    "    for i in range(N):\n",
    "        if i in topk_idx[i]:\n",
    "            correct += 1\n",
    "    return correct / N\n",
    "\n",
    "recall10 = recall_at_k(sim_matrix, k=10)\n",
    "print(f\"Recall@10 = {recall10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ac5c7",
   "metadata": {},
   "source": [
    "## Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c18c1944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "class CodeSearchModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "\n",
    "    def forward(self, code_input_ids, code_attention_mask, query_input_ids, query_attention_mask):\n",
    "        batch_size = code_input_ids.size(0)\n",
    "        combined_input_ids = torch.cat([code_input_ids, query_input_ids], dim=0)\n",
    "        combined_attention_mask = torch.cat([code_attention_mask, query_attention_mask], dim=0)\n",
    "        outputs = self.encoder(input_ids=combined_input_ids, attention_mask=combined_attention_mask)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "        code_emb = F.normalize(cls_emb[:batch_size], p=2, dim=1)\n",
    "        query_emb = F.normalize(cls_emb[batch_size:], p=2, dim=1)\n",
    "        return code_emb, query_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a6c25eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeSearchModel(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CodeSearchModel(base_model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732756ee",
   "metadata": {},
   "source": [
    "## Prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a31e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeSearchDataset(Dataset):\n",
    "    def __init__(self, csv_path, tokenizer, max_len=256):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.queries = df[\"query\"].tolist()\n",
    "        self.codes = df[\"code\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = str(self.queries[idx])\n",
    "        code = str(self.codes[idx])\n",
    "        code_enc = self.tokenizer(code, padding='max_length', truncation=True,\n",
    "                                  max_length=self.max_len, return_tensors=\"pt\")\n",
    "        query_enc = self.tokenizer(query, padding='max_length', truncation=True,\n",
    "                                   max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"code_input_ids\": code_enc[\"input_ids\"].squeeze(0),\n",
    "            \"code_attention_mask\": code_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"query_input_ids\": query_enc[\"input_ids\"].squeeze(0),\n",
    "            \"query_attention_mask\": query_enc[\"attention_mask\"].squeeze(0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ae392",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a75a29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(code_emb, query_emb, temperature=0.05):\n",
    "    sim_matrix = torch.matmul(query_emb, code_emb.T) / temperature  # (B, B)\n",
    "    labels = torch.arange(sim_matrix.size(0)).to(sim_matrix.device)\n",
    "    return nn.CrossEntropyLoss()(sim_matrix, labels)\n",
    "\n",
    "def recall_at_k(sim_matrix, k=10):\n",
    "    N = sim_matrix.size(0)\n",
    "    _, topk_idx = torch.topk(sim_matrix, k, dim=1)\n",
    "    correct = sum(i in topk_idx[i] for i in range(N))\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecf43a1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list, model, tokenizer, batch_size=16, max_len=256):\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Embedding\"):\n",
    "        batch_text = text_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_text, padding=True, truncation=True,\n",
    "                           max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model.encoder(**inputs).last_hidden_state[:, 0, :]\n",
    "            embs.append(F.normalize(out, p=2, dim=1))\n",
    "    return torch.cat(embs, dim=0)\n",
    "\n",
    "def train_model(model, tokenizer, train_csv, epochs=10, batch_size=8, lr=2e-5, max_len=256):\n",
    "    dataset = CodeSearchDataset(train_csv, tokenizer, max_len=max_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_train_loss = float(\"inf\")\n",
    "    best_model_path = \"best_model.pt\"\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            code_input_ids = batch[\"code_input_ids\"].to(device)\n",
    "            code_attention_mask = batch[\"code_attention_mask\"].to(device)\n",
    "            query_input_ids = batch[\"query_input_ids\"].to(device)\n",
    "            query_attention_mask = batch[\"query_attention_mask\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            code_emb, query_emb = model(\n",
    "                code_input_ids, code_attention_mask,\n",
    "                query_input_ids, query_attention_mask\n",
    "            )\n",
    "            loss = contrastive_loss(code_emb, query_emb, temperature=0.05)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} - Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_train_loss:\n",
    "            best_train_loss = avg_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Saved new best model (train_loss={avg_loss:.4f})\")\n",
    "\n",
    "    print(f\"Training complete. Best Train Loss: {best_train_loss:.4f}\")\n",
    "    return best_model_path\n",
    "\n",
    "train_csv = \"data/train_queries.csv\"\n",
    "model = CodeSearchModel(base_model)\n",
    "best_model_path = train_model(model, tokenizer, train_csv, epochs=20, batch_size=8, lr=2e-5)\n",
    "\n",
    "print(\"\\nEvaluating Recall@10 for Best Model ...\")\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def format_str(string):\n",
    "    for char in ['\\r\\n', '\\r', '\\n']:\n",
    "        string = string.replace(char, ' ')\n",
    "    return string.strip()\n",
    "\n",
    "df = pd.read_csv(train_csv)\n",
    "codes = [format_str(c)[:512] for c in df[\"code\"]]\n",
    "queries = [format_str(q)[:512] for q in df[\"query\"]]\n",
    "\n",
    "query_embs = get_embeddings(queries, model, tokenizer)\n",
    "code_embs = get_embeddings(codes, model, tokenizer)\n",
    "\n",
    "sim_matrix = torch.matmul(query_embs, code_embs.T)\n",
    "recall10 = recall_at_k(sim_matrix, k=10)\n",
    "print(f\"Best Model Recall@10 = {recall10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9eddc9",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d27f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "class CodeSearchModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "\n",
    "    def forward(self, code_input_ids, code_attention_mask, query_input_ids, query_attention_mask):\n",
    "        batch_size = code_input_ids.size(0)\n",
    "        combined_input_ids = torch.cat([code_input_ids, query_input_ids], dim=0)\n",
    "        combined_attention_mask = torch.cat([code_attention_mask, query_attention_mask], dim=0)\n",
    "        outputs = self.encoder(input_ids=combined_input_ids, attention_mask=combined_attention_mask)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "        code_emb = F.normalize(cls_emb[:batch_size], p=2, dim=1)\n",
    "        query_emb = F.normalize(cls_emb[batch_size:], p=2, dim=1)\n",
    "        return code_emb, query_emb\n",
    "\n",
    "def get_embeddings(text_list, model, tokenizer, batch_size=16, max_len=256):\n",
    "    model.eval()\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Embedding\"):\n",
    "        batch_text = text_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_text, padding=True, truncation=True,\n",
    "                           max_length=max_len, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model.encoder(**inputs).last_hidden_state[:, 0, :]\n",
    "            emb = F.normalize(out, p=2, dim=1)\n",
    "            all_embs.append(emb)\n",
    "    return torch.cat(all_embs, dim=0)\n",
    "\n",
    "def run_inference(model_path, query_csv, code_csv, output_csv=\"submission.csv\", top_k=10):\n",
    "    # 載入 fine-tuned 模型\n",
    "    model = CodeSearchModel(base_model)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    df = pd.read_csv(query_csv)\n",
    "    queries = [format_str(q)[:512] for q in df[\"query\"]]\n",
    "    df = pd.read_csv(code_csv)\n",
    "    codes = [format_str(c)[:512] for c in df[\"code\"]]\n",
    "\n",
    "    print(f\"Loaded {len(queries)} queries and {len(codes)} code snippets\")\n",
    "\n",
    "    query_embs = get_embeddings(queries, model, tokenizer)\n",
    "    code_embs = get_embeddings(codes, model, tokenizer)\n",
    "\n",
    "    print(\"Calculating similarity matrix ...\")\n",
    "    sim_matrix = torch.matmul(query_embs, code_embs.T)\n",
    "\n",
    "    _, topk_indices = torch.topk(sim_matrix, k=top_k, dim=1)\n",
    "    results = topk_indices.cpu().numpy().tolist()\n",
    "\n",
    "    results = [[idx + 1 for idx in code_ids] for code_ids in results]\n",
    "\n",
    "    output_submission_file(results, output_csv)\n",
    "    print(f\"✅ Saved results to {output_csv}\")\n",
    "\n",
    "\n",
    "\n",
    "model_path = \"best_model.pt\"       \n",
    "query_csv = \"data/test_queries.csv\"          \n",
    "code_csv = \"data/code_snippets.csv\"\n",
    "output_csv = \"result/fine_tuned_submission.csv\"\n",
    "run_inference(model_path, query_csv, code_csv, output_csv, top_k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
