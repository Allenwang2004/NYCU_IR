{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b564d7",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1ba5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_code(code):\n",
    "    code = code.strip()\n",
    "    lines = [line.rstrip() for line in code.splitlines() if line.strip() != \"\"]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def clean_text(s):\n",
    "    if isinstance(s, str):\n",
    "        s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        s = \" \".join(s.split())  # 去多餘空白\n",
    "    return s\n",
    "\n",
    "def load_code_data(file_path):\n",
    "    code_snippets_df = pd.read_csv(file_path, engine='python')\n",
    "    code_snippets = code_snippets_df['code'].tolist()\n",
    "    return code_snippets\n",
    "\n",
    "def load_query_data(file_path):\n",
    "    query_snippets_df = pd.read_csv(file_path, engine='python')\n",
    "    query_snippets = query_snippets_df['query'].tolist()\n",
    "    return query_snippets\n",
    "\n",
    "def output_submission_file(results, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"query_id,code_id\\n\")\n",
    "        for query_id, code_ids in enumerate(results):\n",
    "            code_ids_str = ' '.join(map(str, code_ids))\n",
    "            f.write(f\"{query_id+1},{code_ids_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60295bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class TFIDF:\n",
    "    '''\n",
    "    documents: List of documents (strings)\n",
    "    term_freqs: List of term frequency dictionaries for each document ex: [{'term1': 2, 'term2': 1}, ...]\n",
    "    doc_freqs: Document frequency dictionary for terms ex: {'term1': 3, 'term2': 5}\n",
    "    num_docs: Total number of documents\n",
    "    vocabulary: Set of unique terms across all documents\n",
    "    idf: Inverse Document Frequency dictionary for terms\n",
    "    tfidf_matrix: 2D numpy array storing TF-IDF scores for documents\n",
    "    vocab_to_idx: Mapping from term to its index in the vocabulary\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.term_freqs = []\n",
    "        self.doc_freqs = defaultdict(int)\n",
    "        self.num_docs = 0\n",
    "        self.vocabulary = set()\n",
    "        self.idf = defaultdict(float)\n",
    "        self.tfidf_matrix = None\n",
    "        self.vocab_to_idx = defaultdict(int)\n",
    "\n",
    "    def add_document(self, document):\n",
    "        self.documents.append(document)\n",
    "        self.num_docs += 1\n",
    "        term_count = defaultdict(int)\n",
    "\n",
    "        for term in document.split():\n",
    "            term_count[term] += 1\n",
    "            self.vocabulary.add(term)\n",
    "\n",
    "        self.term_freqs.append(term_count)\n",
    "\n",
    "        for term in term_count.keys():\n",
    "            self.doc_freqs[term] += 1\n",
    "    \n",
    "    def _build_vocab_index(self):\n",
    "        self.vocabulary = list(self.vocabulary)\n",
    "        self.vocab_to_idx = {term: idx for idx, term in enumerate(self.vocabulary)}\n",
    "        \n",
    "    def compute_tfidf(self):\n",
    "        if self.tfidf_matrix is not None:\n",
    "            return self.tfidf_matrix\n",
    "            \n",
    "        self._build_vocab_index()\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        for term in self.vocabulary:\n",
    "            self.idf[term] = math.log((1 + (self.num_docs/self.doc_freqs[term])), 2)\n",
    "        \n",
    "        self.tfidf_matrix = np.zeros((self.num_docs, vocab_size))\n",
    "        \n",
    "        for doc_idx, term_count in enumerate(self.term_freqs):\n",
    "            for term, count in term_count.items():\n",
    "                if term in self.vocab_to_idx:\n",
    "                    tf = 1 + math.log(count, 2)\n",
    "                    tfidf_val = tf * self.idf[term]\n",
    "                    self.tfidf_matrix[doc_idx][self.vocab_to_idx[term]] = tfidf_val\n",
    "        \n",
    "        return self.tfidf_matrix\n",
    "\n",
    "    def get_query_vector(self, query):\n",
    "        \n",
    "        if self.tfidf_matrix is None:\n",
    "            self.compute_tfidf()\n",
    "        \n",
    "        query_term_count = defaultdict(int)\n",
    "        for term in query.split():\n",
    "            query_term_count[term] += 1\n",
    "\n",
    "        query_vector = np.zeros(len(self.vocabulary))\n",
    "        \n",
    "        for term, count in query_term_count.items():\n",
    "            if term in self.vocab_to_idx:\n",
    "                tf = 1 + math.log(count, 2)\n",
    "                idf = self.idf.get(term, 0.0)\n",
    "                query_vector[self.vocab_to_idx[term]] = tf * idf\n",
    "                \n",
    "        return query_vector\n",
    "    \n",
    "    def compute_similarity_batch(self, query_vector):\n",
    "        \n",
    "        dot_products = np.dot(self.tfidf_matrix, query_vector)\n",
    "        \n",
    "        # Document Length Normalization\n",
    "        doc_norms = np.linalg.norm(self.tfidf_matrix, axis=1)\n",
    "        query_norm = np.linalg.norm(query_vector)\n",
    "        \n",
    "        valid_mask = (doc_norms > 0) & (query_norm > 0)\n",
    "        similarities = np.zeros(len(doc_norms))\n",
    "        similarities[valid_mask] = dot_products[valid_mask] / (doc_norms[valid_mask] * query_norm)\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def get_top_k_similar_documents(self, query, k):\n",
    "        \n",
    "        query_vector = self.get_query_vector(query)\n",
    "        similarities = self.compute_similarity_batch(query_vector)\n",
    "        \n",
    "        if k >= len(similarities):\n",
    "            top_k_indices = np.argsort(similarities)[::-1]\n",
    "        else:\n",
    "            top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
    "            top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
    "        \n",
    "        return top_k_indices.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e32fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        self.documents = []\n",
    "        self.term_freqs = []\n",
    "        self.doc_freqs = defaultdict(int)\n",
    "        self.num_docs = 0\n",
    "        self.vocabulary = set()\n",
    "        self.doc_lengths = []\n",
    "        self.avg_doc_len = 0\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self._precomputed_idf = {}\n",
    "        self._precomputed_norms = None\n",
    "\n",
    "    def add_document(self, document):\n",
    "        self.documents.append(document)\n",
    "        terms = document.split()\n",
    "        self.doc_lengths.append(len(terms))\n",
    "        self.num_docs += 1\n",
    "        term_count = defaultdict(int)\n",
    "\n",
    "        for term in terms:\n",
    "            term_count[term] += 1\n",
    "            self.vocabulary.add(term)\n",
    "\n",
    "        self.term_freqs.append(term_count)\n",
    "\n",
    "        for term in term_count.keys():\n",
    "            self.doc_freqs[term] += 1\n",
    "    \n",
    "    def _precompute_idf(self):\n",
    "        \"\"\"預計算所有詞彙的 IDF 值\"\"\"\n",
    "        self.avg_doc_len = sum(self.doc_lengths) / self.num_docs\n",
    "        for term in self.vocabulary:\n",
    "            df = self.doc_freqs[term]\n",
    "            self._precomputed_idf[term] = math.log((self.num_docs - df + 0.5) / (df + 0.5))\n",
    "    \n",
    "    def _precompute_normalization_factors(self):\n",
    "        \"\"\"預計算正規化因子\"\"\"\n",
    "        self._precomputed_norms = np.array([\n",
    "            self.k1 * (1 - self.b + self.b * (doc_len / self.avg_doc_len))\n",
    "            for doc_len in self.doc_lengths\n",
    "        ])\n",
    "    \n",
    "    def compute_similarity_batch(self, query):\n",
    "        if not self._precomputed_idf:\n",
    "            self._precompute_idf()\n",
    "        if self._precomputed_norms is None:\n",
    "            self._precompute_normalization_factors()\n",
    "            \n",
    "        query_terms = query.split()\n",
    "        similarities = np.zeros(self.num_docs)\n",
    "        \n",
    "        for doc_idx in range(self.num_docs):\n",
    "            score = 0.0\n",
    "            term_freq_doc = self.term_freqs[doc_idx]\n",
    "            norm_factor = self._precomputed_norms[doc_idx]\n",
    "            \n",
    "            for term in query_terms:\n",
    "                if term in self.vocabulary:\n",
    "                    tf = term_freq_doc.get(term, 0)\n",
    "                    if tf > 0:  # 只計算有出現的詞彙\n",
    "                        idf = self._precomputed_idf[term]\n",
    "                        normalized_tf = (tf * (self.k1 + 1)) / (tf + norm_factor)\n",
    "                        score += idf * normalized_tf\n",
    "            \n",
    "            similarities[doc_idx] = score\n",
    "\n",
    "        return similarities\n",
    "\n",
    "    def get_top_k_similar_documents(self, query, k):\n",
    "        similarities = self.compute_similarity_batch(query)\n",
    "\n",
    "        if k >= len(similarities):\n",
    "            top_k_indices = np.argsort(similarities)[::-1]      \n",
    "        else:\n",
    "            top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
    "            top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
    "        return top_k_indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f9001",
   "metadata": {},
   "source": [
    "# Sparse Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b930fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file_path = 'data/test_queries.csv'\n",
    "code_file_path = 'data/code_snippets.csv'\n",
    "\n",
    "code_snippets = load_code_data(code_file_path)\n",
    "queries = load_query_data(query_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd3a4f",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7ecba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TFIDF()\n",
    "\n",
    "for code in code_snippets:\n",
    "    tfidf.add_document(code)\n",
    "    \n",
    "tfidf.compute_tfidf()\n",
    "\n",
    "top_k_results = []\n",
    "for query in queries:\n",
    "    top_k = tfidf.get_top_k_similar_documents(query, 10)\n",
    "    top_k_results.append(top_k)\n",
    "\n",
    "output_submission_file(top_k_results, 'tfi_df_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b74cc",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9dce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25()\n",
    "\n",
    "for code in code_snippets:\n",
    "    bm25.add_document(code)\n",
    "\n",
    "top_k_results = []\n",
    "for query in queries:\n",
    "    top_k = bm25.get_top_k_similar_documents(query, 10)\n",
    "    top_k_results.append(top_k)\n",
    "\n",
    "output_submission_file(top_k_results, 'bm25_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42323e28",
   "metadata": {},
   "source": [
    "# Dense Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a17d32",
   "metadata": {},
   "source": [
    "## Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "803c2c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (763 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 500/500 [00:00<00:00, 5960.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of code snippets: 135.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 10413.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of queries: 61.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:26<00:00,  1.67s/it]\n",
      "100%|██████████| 16/16 [00:13<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "code_df = pd.read_csv(\"data/code_snippets.csv\")\n",
    "query_df = pd.read_csv(\"data/test_queries.csv\")\n",
    "\n",
    "code_df[\"code\"] = code_df[\"code\"].map(clean_text)\n",
    "query_df[\"query\"] = query_df[\"query\"].map(clean_text)\n",
    "\n",
    "codes = code_df[\"code\"].tolist()\n",
    "queries = query_df[\"query\"].tolist()\n",
    "\n",
    "lens = [len(tokenizer(c, truncation=False)['input_ids']) for c in tqdm(codes)]\n",
    "print(\"Average length of code snippets:\", np.mean(lens))\n",
    "\n",
    "lens = [len(tokenizer(q, truncation=False)['input_ids']) for q in tqdm(queries)]\n",
    "print(\"Average length of queries:\", np.mean(lens))\n",
    "\n",
    "def encode_codes(texts, max_len=256, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        batch_embeddings = model_output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def encode_texts(texts, max_len=128, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        batch_embeddings = model_output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "code_embeds = encode_codes(codes)\n",
    "query_embeds = encode_texts(queries)\n",
    "\n",
    "code_embeds = normalize(code_embeds)\n",
    "query_embeds = normalize(query_embeds)\n",
    "\n",
    "similarity_matrix = cosine_similarity(query_embeds, code_embeds)\n",
    "\n",
    "top_k = 10\n",
    "results = []\n",
    "for i in range(len(queries)):\n",
    "    top_indices = similarity_matrix[i].argsort()[-top_k:][::-1]\n",
    "    results.append(top_indices.tolist())\n",
    "\n",
    "output_submission_file(results, 'pre_trained_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ac5c7",
   "metadata": {},
   "source": [
    "## Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6b0e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading training data...\n",
      "Training data shape: (500, 2)\n",
      "Generating negative samples...\n",
      "Loading training data...\n",
      "Training data shape: (500, 2)\n",
      "Generating negative samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 681778.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 1000 (positive: 500, negative: 500)\n",
      "Train samples: 800, Validation samples: 200\n",
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune a pre-trained model on a data/train_queries.csv which contains code snippets and their corresponding queries.\n",
    "# data only contain code and query\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW  # AdamW 現在在 torch.optim 中\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# 設定隨機種子\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 檢查設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 載入預訓練模型\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "class CodeQuerySimilarityModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "        self.similarity_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, code_input_ids, code_attention_mask, query_input_ids, query_attention_mask):\n",
    "        # 編碼 code\n",
    "        code_outputs = self.encoder(input_ids=code_input_ids, attention_mask=code_attention_mask)\n",
    "        code_embed = code_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        \n",
    "        # 編碼 query\n",
    "        query_outputs = self.encoder(input_ids=query_input_ids, attention_mask=query_attention_mask)\n",
    "        query_embed = query_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        \n",
    "        # 組合特徵\n",
    "        combined = torch.cat([code_embed, query_embed], dim=-1)\n",
    "        similarity = self.similarity_head(combined)\n",
    "        \n",
    "        return similarity.squeeze(), code_embed, query_embed\n",
    "\n",
    "class CodeQueryDataset(Dataset):\n",
    "    def __init__(self, codes, queries, labels, tokenizer, max_code_len=512, max_query_len=128):\n",
    "        self.codes = codes\n",
    "        self.queries = queries\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_code_len = max_code_len\n",
    "        self.max_query_len = max_query_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        code = str(self.codes[idx])\n",
    "        query = str(self.queries[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 編碼 code\n",
    "        code_encoding = self.tokenizer(\n",
    "            code,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_code_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 編碼 query\n",
    "        query_encoding = self.tokenizer(\n",
    "            query,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_query_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'code_input_ids': code_encoding['input_ids'].squeeze(),\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(),\n",
    "            'query_input_ids': query_encoding['input_ids'].squeeze(),\n",
    "            'query_attention_mask': query_encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# 載入訓練資料\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv('data/train_queries.csv')\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "\n",
    "# 清理資料\n",
    "train_df['code'] = train_df['code'].apply(clean_text)\n",
    "train_df['query'] = train_df['query'].apply(clean_text)\n",
    "\n",
    "# 創建正樣本（每個 code-query 對都是相關的）\n",
    "positive_codes = train_df['code'].tolist()\n",
    "positive_queries = train_df['query'].tolist()\n",
    "positive_labels = [1.0] * len(positive_codes)\n",
    "\n",
    "# 創建負樣本（隨機配對不相關的 code-query）\n",
    "negative_codes = []\n",
    "negative_queries = []\n",
    "negative_labels = []\n",
    "\n",
    "print(\"Generating negative samples...\")\n",
    "n_negatives = len(positive_codes)  # 與positive樣本數量相同\n",
    "\n",
    "for i in tqdm(range(n_negatives)):\n",
    "    # 隨機選擇不同的 query\n",
    "    neg_idx = random.randint(0, len(positive_queries) - 1)\n",
    "    while neg_idx == i:  # 確保不是同一對\n",
    "        neg_idx = random.randint(0, len(positive_queries) - 1)\n",
    "    \n",
    "    negative_codes.append(positive_codes[i])\n",
    "    negative_queries.append(positive_queries[neg_idx])\n",
    "    negative_labels.append(0.0)\n",
    "\n",
    "# 合併正負樣本\n",
    "all_codes = positive_codes + negative_codes\n",
    "all_queries = positive_queries + negative_queries\n",
    "all_labels = positive_labels + negative_labels\n",
    "\n",
    "# 打亂資料\n",
    "indices = list(range(len(all_codes)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "all_codes = [all_codes[i] for i in indices]\n",
    "all_queries = [all_queries[i] for i in indices]\n",
    "all_labels = [all_labels[i] for i in indices]\n",
    "\n",
    "print(f\"Total training samples: {len(all_codes)} (positive: {len(positive_codes)}, negative: {len(negative_codes)})\")\n",
    "\n",
    "# 分割訓練集和驗證集\n",
    "split_idx = int(0.8 * len(all_codes))\n",
    "train_codes = all_codes[:split_idx]\n",
    "train_queries = all_queries[:split_idx]\n",
    "train_labels = all_labels[:split_idx]\n",
    "\n",
    "val_codes = all_codes[split_idx:]\n",
    "val_queries = all_queries[split_idx:]\n",
    "val_labels = all_labels[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_codes)}, Validation samples: {len(val_codes)}\")\n",
    "\n",
    "# 創建資料集\n",
    "train_dataset = CodeQueryDataset(train_codes, train_queries, train_labels, tokenizer)\n",
    "val_dataset = CodeQueryDataset(val_codes, val_queries, val_labels, tokenizer)\n",
    "\n",
    "# 創建資料載入器\n",
    "batch_size = 8  # 根據GPU記憶體調整\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型\n",
    "model = CodeQuerySimilarityModel(base_model)\n",
    "model.to(device)\n",
    "\n",
    "# 優化器和學習率排程\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# 損失函數\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "print(\"Starting fine-tuning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "775787b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 27/100 [03:05<08:21,  6.87s/it, loss=0.7104, acc=0.5139]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# 訓練\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# 驗證\u001b[39;00m\n\u001b[32m     93\u001b[39m val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, scheduler, criterion, device)\u001b[39m\n\u001b[32m     25\u001b[39m loss = criterion(predictions, labels)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 反向傳播\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     30\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 訓練循環\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # 移到GPU\n",
    "        code_input_ids = batch['code_input_ids'].to(device)\n",
    "        code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "        query_input_ids = batch['query_input_ids'].to(device)\n",
    "        query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # 前向傳播\n",
    "        optimizer.zero_grad()\n",
    "        predictions, code_embeds, query_embeds = model(\n",
    "            code_input_ids, code_attention_mask,\n",
    "            query_input_ids, query_attention_mask\n",
    "        )\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 反向傳播\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 統計\n",
    "        total_loss += loss.item()\n",
    "        predicted_labels = (predictions > 0.5).float()\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct_predictions/total_predictions:.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            query_input_ids = batch['query_input_ids'].to(device)\n",
    "            query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            predictions, _, _ = model(\n",
    "                code_input_ids, code_attention_mask,\n",
    "                query_input_ids, query_attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 訓練模型\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 訓練\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
    "    \n",
    "    # 驗證\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # 記錄結果\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Saved best model!\")\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入最佳模型並進行推論\n",
    "print(\"Loading best model for inference...\")\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "def encode_texts_finetuned(texts, max_len=512, batch_size=16):\n",
    "    \"\"\"使用 fine-tuned 模型編碼文本\"\"\"\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # 使用編碼器部分獲取embeddings\n",
    "            outputs = model.encoder(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# 編碼所有代碼片段和查詢\n",
    "print(\"Encoding code snippets with fine-tuned model...\")\n",
    "code_embeddings_ft = encode_texts_finetuned(codes, max_len=512, batch_size=8)\n",
    "\n",
    "print(\"Encoding queries with fine-tuned model...\")\n",
    "query_embeddings_ft = encode_texts_finetuned(queries, max_len=128, batch_size=16)\n",
    "\n",
    "print(f\"Code embeddings shape: {code_embeddings_ft.shape}\")\n",
    "print(f\"Query embeddings shape: {query_embeddings_ft.shape}\")\n",
    "\n",
    "# 正規化embeddings\n",
    "from sklearn.preprocessing import normalize\n",
    "code_embeddings_ft_norm = normalize(code_embeddings_ft, norm='l2')\n",
    "query_embeddings_ft_norm = normalize(query_embeddings_ft, norm='l2')\n",
    "\n",
    "# 計算相似度矩陣\n",
    "print(\"Computing similarity matrix...\")\n",
    "similarity_matrix_ft = np.dot(query_embeddings_ft_norm, code_embeddings_ft_norm.T)\n",
    "\n",
    "# 獲取每個查詢的top-10結果\n",
    "top_k = 10\n",
    "results_ft = []\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    if top_k >= len(codes):\n",
    "        top_indices = np.argsort(similarity_matrix_ft[i])[::-1]\n",
    "    else:\n",
    "        top_indices = np.argpartition(similarity_matrix_ft[i], -top_k)[-top_k:]\n",
    "        top_indices = top_indices[np.argsort(similarity_matrix_ft[i][top_indices])[::-1]]\n",
    "    \n",
    "    results_ft.append(top_indices.tolist())\n",
    "\n",
    "# 輸出結果\n",
    "output_submission_file(results_ft, 'fine_tuned_submission.csv')\n",
    "print(\"Fine-tuned model results saved to fine_tuned_submission.csv\")\n",
    "\n",
    "# 顯示訓練損失曲線\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
