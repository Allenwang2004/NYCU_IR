{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b564d7",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ba5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import html\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "_whitespace_re = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "_word_re = re.compile(r\"[A-Za-z_][A-Za-z0-9_]*\")\n",
    "# Punctuation/delimiters typical for C/Java/JS/Solidity/Python\n",
    "# We'll split code tokens by placing spaces around these, then split.\n",
    "_separators = r\"[\\(\\)\\[\\]\\{\\};:,\\.\\+\\-\\*/%&\\|\\^!=<>\\?~]\"\n",
    "_sep_re = re.compile(f\"({_separators})\")\n",
    "\n",
    "# String literal patterns (\"...\", '...', `...`)\n",
    "_str_re = re.compile(r'(\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\'|\".*?\"|\\'.*?\\'|`.*?`)', re.DOTALL)\n",
    "_num_re = re.compile(r\"\\b\\d+(?:_\\d+)*(?:\\.[0-9_]+)?\\b\")\n",
    "\n",
    "# Comments (C-like): // line, /* block */\n",
    "_c_line_cmt = re.compile(r\"//.*?(?=\\n|$)\")\n",
    "_c_block_cmt = re.compile(r\"/\\*.*?\\*/\", re.DOTALL)\n",
    "# Python: # line, triple-quoted docstrings (we'll treat as comments)\n",
    "_py_line_cmt = re.compile(r\"#.*?(?=\\n|$)\")\n",
    "_py_doc_cmt = re.compile(r'(\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\')', re.DOTALL)\n",
    "\n",
    "_c_like_file_ext = {\".c\", \".h\", \".cpp\", \".hpp\", \".cc\", \".java\", \".js\", \".ts\", \".sol\", \".cs\", \".swift\", \".go\"}\n",
    "_py_like_file_ext = {\".py\"}\n",
    "\n",
    "@dataclass\n",
    "class PreprocessConfig:\n",
    "    lowercase: bool = True\n",
    "    normalize_numbers: bool = True   # replace numbers with <NUM>\n",
    "    normalize_strings: bool = True   # replace strings with <STR>\n",
    "    split_identifiers: bool = True\n",
    "    keep_empty: bool = False\n",
    "\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    return _whitespace_re.sub(\" \", text).strip()\n",
    "\n",
    "\n",
    "def split_identifiers(token: str) -> List[str]:\n",
    "    \"\"\"Split camelCase/mixedCase and snake_case identifiers.\n",
    "    e.g. `allowedAmount_total` -> [\"allowed\", \"Amount\", \"total\"] -> lowercased later\n",
    "    \"\"\"\n",
    "    # First split snake_case\n",
    "    parts = re.split(r\"_+\", token)\n",
    "    out: List[str] = []\n",
    "    for p in parts:\n",
    "        # Split on camelCase transitions (including digits)\n",
    "        # e.g. HTTPServerError -> HTTP, Server, Error\n",
    "        for m in re.finditer(r\"[A-Z]?[a-z]+|[A-Z]+(?![a-z])|\\d+\", p):\n",
    "            out.append(m.group(0))\n",
    "    return [t for t in out if t]\n",
    "\n",
    "\n",
    "def extract_comments_and_code(text: str, language_hint: str = \"auto\") -> Tuple[str, str]:\n",
    "    \"\"\"Return (comments_text, code_without_comments).\n",
    "    Supports a broad regex-based pass for C-like and Python-like languages.\n",
    "    \"\"\"\n",
    "    s = text\n",
    "    comments: List[str] = []\n",
    "\n",
    "    # Python-like docstrings first to avoid gobbling by string regex\n",
    "    for m in _py_doc_cmt.finditer(s):\n",
    "        comments.append(m.group(0))\n",
    "    s = _py_doc_cmt.sub(\"\\n\", s)\n",
    "\n",
    "    # C-like block comments\n",
    "    for m in _c_block_cmt.finditer(s):\n",
    "        comments.append(m.group(0))\n",
    "    s = _c_block_cmt.sub(\"\\n\", s)\n",
    "\n",
    "    # Line comments\n",
    "    for m in _c_line_cmt.finditer(s):\n",
    "        comments.append(m.group(0))\n",
    "    s = _c_line_cmt.sub(\"\\n\", s)\n",
    "\n",
    "    for m in _py_line_cmt.finditer(s):\n",
    "        comments.append(m.group(0))\n",
    "    s = _py_line_cmt.sub(\"\\n\", s)\n",
    "\n",
    "    comments_text = \"\\n\".join(comments)\n",
    "    code_wo_comments = s\n",
    "    return comments_text, code_wo_comments\n",
    "\n",
    "\n",
    "def tokenize_comment_text(text: str, cfg: PreprocessConfig) -> List[str]:\n",
    "    text = html.unescape(text)\n",
    "    if cfg.normalize_strings:\n",
    "        text = _str_re.sub(\" <STR> \", text)\n",
    "    if cfg.normalize_numbers:\n",
    "        text = _num_re.sub(\" <NUM> \", text)\n",
    "    # Words only for comments; punctuation is less useful\n",
    "    toks = _word_re.findall(text)\n",
    "    if cfg.split_identifiers:\n",
    "        expanded: List[str] = []\n",
    "        for tok in toks:\n",
    "            expanded.extend(split_identifiers(tok))\n",
    "        toks = expanded\n",
    "    if cfg.lowercase:\n",
    "        toks = [t.lower() for t in toks]\n",
    "    return toks if toks or cfg.keep_empty else (toks or [])\n",
    "\n",
    "\n",
    "def tokenize_code(text: str, cfg: PreprocessConfig) -> List[str]:\n",
    "    text = html.unescape(text)\n",
    "    # Replace string and numbers with placeholders to reduce noise\n",
    "    if cfg.normalize_strings:\n",
    "        text = _str_re.sub(\" <STR> \", text)\n",
    "    if cfg.normalize_numbers:\n",
    "        text = _num_re.sub(\" <NUM> \", text)\n",
    "    # Put spaces around separators so we can split and KEEP them\n",
    "    text = _sep_re.sub(r\" \\1 \", text)\n",
    "    # Collapse whitespace\n",
    "    text = normalize_whitespace(text)\n",
    "    raw_toks = text.split(\" \") if text else []\n",
    "\n",
    "    toks: List[str] = []\n",
    "    for tok in raw_toks:\n",
    "        if not tok:\n",
    "            continue\n",
    "        if tok == \"<STR>\" or tok == \"<NUM>\" or _sep_re.fullmatch(tok):\n",
    "            toks.append(tok)\n",
    "        else:\n",
    "            if cfg.split_identifiers:\n",
    "                toks.extend(split_identifiers(tok))\n",
    "            else:\n",
    "                toks.append(tok)\n",
    "    if cfg.lowercase:\n",
    "        toks = [t.lower() for t in toks]\n",
    "    return toks if toks or cfg.keep_empty else (toks or [])\n",
    "\n",
    "\n",
    "def preprocess_code_snippet(snippet: str, cfg: PreprocessConfig) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Given raw code snippet, return (comment_tokens, code_tokens).\"\"\"\n",
    "    comments_text, code_wo = extract_comments_and_code(snippet)\n",
    "    cmt_toks = tokenize_comment_text(comments_text, cfg)\n",
    "    code_toks = tokenize_code(code_wo, cfg)\n",
    "    return cmt_toks, code_toks\n",
    "\n",
    "\n",
    "def preprocess_query(q: str, cfg: PreprocessConfig) -> List[str]:\n",
    "    # For queries, treat like comment text (natural language)\n",
    "    return tokenize_comment_text(q, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31b9237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_csv = \"data/code_snippets.csv\"\n",
    "queries_csv = \"data/test_queries.csv\"\n",
    "out_codes_csv = \"data/code_snippets_proc.csv\"\n",
    "out_queries_csv = \"data/test_queries_proc.csv\"\n",
    "\n",
    "cfg = PreprocessConfig(\n",
    "        lowercase=True,\n",
    "        split_identifiers=True,\n",
    "        normalize_numbers=True,\n",
    "        normalize_strings=True,\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(codes_csv, engine=\"python\")\n",
    "if \"code\" not in df.columns:\n",
    "    raise ValueError(\"codes CSV must contain a 'code' column\")\n",
    "comment_tokens_list: List[List[str]] = []\n",
    "code_tokens_list: List[List[str]] = []\n",
    "all_tokens_list: List[List[str]] = []\n",
    "\n",
    "for s in df[\"code\"].astype(str).tolist():\n",
    "    cmt_toks, code_toks = preprocess_code_snippet(s, cfg)\n",
    "    comment_tokens_list.append(cmt_toks)\n",
    "    code_tokens_list.append(code_toks)\n",
    "    all_tokens_list.append(cmt_toks + code_toks)\n",
    "\n",
    "# df[\"comment_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in comment_tokens_list]\n",
    "df[\"code_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in code_tokens_list]\n",
    "# df[\"all_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in all_tokens_list]\n",
    "df.to_csv(out_codes_csv, index=False)\n",
    "\n",
    "# Process queries\n",
    "df = pd.read_csv(queries_csv, engine=\"python\")\n",
    "if \"query\" not in df.columns:\n",
    "    raise ValueError(\"queries CSV must contain a 'query' column\")\n",
    "query_tokens_list: List[List[str]] = []\n",
    "for s in df[\"query\"].astype(str).tolist():\n",
    "    q_toks = preprocess_query(s, cfg)\n",
    "    query_tokens_list.append(q_toks)\n",
    "df[\"query_tokens\"] = [json.dumps(x, ensure_ascii=False) for x in query_tokens_list]\n",
    "df.to_csv(out_queries_csv, index=False)\n",
    "\n",
    "code_snippets = pd.read_csv(out_codes_csv, engine=\"python\")['code_tokens']\n",
    "queries = pd.read_csv(out_queries_csv, engine=\"python\")['query_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "277c74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_submission_file(results, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"query_id,code_id\\n\")\n",
    "        for query_id, code_ids in enumerate(results):\n",
    "            code_ids_str = ' '.join(map(str, code_ids))\n",
    "            f.write(f\"{query_id+1},{code_ids_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b47891",
   "metadata": {},
   "source": [
    "# Sparse Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba89168",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "60295bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "class TFIDF:\n",
    "    '''\n",
    "    documents: List of documents (strings)\n",
    "    term_freqs: List of term frequency dictionaries for each document ex: [{'term1': 2, 'term2': 1}, ...]\n",
    "    doc_freqs: Document frequency dictionary for terms ex: {'term1': 3, 'term2': 5}\n",
    "    num_docs: Total number of documents\n",
    "    vocabulary: Set of unique terms across all documents\n",
    "    idf: Inverse Document Frequency dictionary for terms\n",
    "    tfidf_matrix: 2D numpy array storing TF-IDF scores for documents\n",
    "    vocab_to_idx: Mapping from term to its index in the vocabulary\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.term_freqs = []\n",
    "        self.doc_freqs = defaultdict(int)\n",
    "        self.num_docs = 0\n",
    "        self.vocabulary = set()\n",
    "        self.idf = defaultdict(float)\n",
    "        self.tfidf_matrix = None\n",
    "        self.vocab_to_idx = defaultdict(int)\n",
    "\n",
    "    def add_document(self, document_tokens):\n",
    "        if isinstance(document_tokens, str):\n",
    "            try:\n",
    "                tokens = json.loads(document_tokens)\n",
    "            except json.JSONDecodeError:\n",
    "                \n",
    "                tokens = document_tokens.split()\n",
    "        else:\n",
    "            tokens = document_tokens\n",
    "        self.documents.append(tokens)\n",
    "        self.num_docs += 1\n",
    "        term_count = defaultdict(int)\n",
    "\n",
    "        for term in tokens:\n",
    "            term_count[term] += 1\n",
    "            self.vocabulary.add(term)\n",
    "\n",
    "        self.term_freqs.append(term_count)\n",
    "\n",
    "        for term in term_count.keys():\n",
    "            self.doc_freqs[term] += 1\n",
    "    \n",
    "    def _build_vocab_index(self):\n",
    "        self.vocabulary = list(self.vocabulary)\n",
    "        self.vocab_to_idx = {term: idx for idx, term in enumerate(self.vocabulary)}\n",
    "        \n",
    "    def compute_tfidf(self):\n",
    "        if self.tfidf_matrix is not None:\n",
    "            return self.tfidf_matrix\n",
    "            \n",
    "        self._build_vocab_index()\n",
    "        vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        for term in self.vocabulary:\n",
    "            self.idf[term] = math.log((1 + (self.num_docs/self.doc_freqs[term])), 2)\n",
    "        \n",
    "        self.tfidf_matrix = np.zeros((self.num_docs, vocab_size))\n",
    "        \n",
    "        for doc_idx, term_count in enumerate(self.term_freqs):\n",
    "            for term, count in term_count.items():\n",
    "                if term in self.vocab_to_idx:\n",
    "                    tf = 1 + math.log(count, 2)\n",
    "                    tfidf_val = tf * self.idf[term]\n",
    "                    self.tfidf_matrix[doc_idx][self.vocab_to_idx[term]] = tfidf_val\n",
    "        \n",
    "        return self.tfidf_matrix\n",
    "\n",
    "    def get_query_vector(self, query_tokens):\n",
    "        \n",
    "        if self.tfidf_matrix is None:\n",
    "            self.compute_tfidf()\n",
    "        \n",
    "        if isinstance(query_tokens, str):\n",
    "            try:\n",
    "                tokens = json.loads(query_tokens)\n",
    "            except json.JSONDecodeError:\n",
    "                tokens = query_tokens.split()\n",
    "        else:\n",
    "            tokens = query_tokens\n",
    "        \n",
    "        query_term_count = defaultdict(int)\n",
    "        for term in tokens:\n",
    "            query_term_count[term] += 1\n",
    "\n",
    "        query_vector = np.zeros(len(self.vocabulary))\n",
    "        \n",
    "        for term, count in query_term_count.items():\n",
    "            if term in self.vocab_to_idx:\n",
    "                tf = 1 + math.log(count, 2)\n",
    "                idf = self.idf.get(term, 0.0)\n",
    "                query_vector[self.vocab_to_idx[term]] = tf * idf\n",
    "                \n",
    "        return query_vector\n",
    "    \n",
    "    def compute_similarity_batch(self, query_vector):\n",
    "        \n",
    "        dot_products = np.dot(self.tfidf_matrix, query_vector)\n",
    "        \n",
    "        # Document Length Normalization\n",
    "        doc_norms = np.linalg.norm(self.tfidf_matrix, axis=1)\n",
    "        query_norm = np.linalg.norm(query_vector)\n",
    "        \n",
    "        valid_mask = (doc_norms > 0) & (query_norm > 0)\n",
    "        similarities = np.zeros(len(doc_norms))\n",
    "        similarities[valid_mask] = dot_products[valid_mask] / (doc_norms[valid_mask] * query_norm)\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def get_top_k_similar_documents(self, query_tokens, k):\n",
    "        \n",
    "        query_vector = self.get_query_vector(query_tokens)\n",
    "        similarities = self.compute_similarity_batch(query_vector)\n",
    "        \n",
    "        if k >= len(similarities):\n",
    "            top_k_indices = np.argsort(similarities)[::-1]\n",
    "        else:\n",
    "            top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
    "            top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
    "        \n",
    "        # need to plus 1 to indices for 1-based indexing\n",
    "        top_k_indices += 1\n",
    "        return top_k_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "805e255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TFIDF()\n",
    "\n",
    "for code in code_snippets:\n",
    "    tfidf.add_document(code)\n",
    "    \n",
    "tfidf.compute_tfidf()\n",
    "\n",
    "top_k_results = []\n",
    "for query in queries:\n",
    "    top_k = tfidf.get_top_k_similar_documents(query, 10)\n",
    "    top_k_results.append(top_k)\n",
    "\n",
    "output_submission_file(top_k_results, 'results/tf_idf_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ca4ab",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e32fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 implementation\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        self.documents = []\n",
    "        self.term_freqs = []\n",
    "        self.doc_freqs = defaultdict(int)\n",
    "        self.num_docs = 0\n",
    "        self.vocabulary = set()\n",
    "        self.doc_lengths = []\n",
    "        self.avg_doc_len = 0\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self._precomputed_idf = {}\n",
    "        self._precomputed_norms = None\n",
    "\n",
    "    def add_document(self, document_tokens):\n",
    "        if isinstance(document_tokens, str):\n",
    "            try:\n",
    "                terms = json.loads(document_tokens)\n",
    "            except json.JSONDecodeError:\n",
    "                terms = document_tokens.split()\n",
    "        else:\n",
    "            terms = document_tokens\n",
    "            \n",
    "        self.documents.append(terms)\n",
    "        self.doc_lengths.append(len(terms))\n",
    "        self.num_docs += 1\n",
    "        term_count = defaultdict(int)\n",
    "\n",
    "        for term in terms:\n",
    "            term_count[term] += 1\n",
    "            self.vocabulary.add(term)\n",
    "\n",
    "        self.term_freqs.append(term_count)\n",
    "\n",
    "        for term in term_count.keys():\n",
    "            self.doc_freqs[term] += 1\n",
    "    \n",
    "    def _precompute_idf(self):\n",
    "        self.avg_doc_len = sum(self.doc_lengths) / self.num_docs\n",
    "        for term in self.vocabulary:\n",
    "            df = self.doc_freqs[term]\n",
    "            self._precomputed_idf[term] = math.log((self.num_docs - df + 0.5) / (df + 0.5))\n",
    "    \n",
    "    def _precompute_normalization_factors(self):\n",
    "        self._precomputed_norms = np.array([\n",
    "            self.k1 * (1 - self.b + self.b * (doc_len / self.avg_doc_len))\n",
    "            for doc_len in self.doc_lengths\n",
    "        ])\n",
    "    \n",
    "    def compute_similarity_batch(self, query_tokens):\n",
    "        if not self._precomputed_idf:\n",
    "            self._precompute_idf()\n",
    "        if self._precomputed_norms is None:\n",
    "            self._precompute_normalization_factors()\n",
    "            \n",
    "        if isinstance(query_tokens, str):\n",
    "            try:\n",
    "                query_terms = json.loads(query_tokens)\n",
    "            except json.JSONDecodeError:\n",
    "                query_terms = query_tokens.split()\n",
    "        else:\n",
    "            query_terms = query_tokens\n",
    "            \n",
    "        similarities = np.zeros(self.num_docs)\n",
    "        \n",
    "        for doc_idx in range(self.num_docs):\n",
    "            score = 0.0\n",
    "            term_freq_doc = self.term_freqs[doc_idx]\n",
    "            norm_factor = self._precomputed_norms[doc_idx]\n",
    "            \n",
    "            for term in query_terms:\n",
    "                if term in self.vocabulary:\n",
    "                    tf = term_freq_doc.get(term, 0)\n",
    "                    if tf > 0:  # 只計算有出現的詞彙\n",
    "                        idf = self._precomputed_idf[term]\n",
    "                        normalized_tf = (tf * (self.k1 + 1)) / (tf + norm_factor)\n",
    "                        score += idf * normalized_tf\n",
    "            \n",
    "            similarities[doc_idx] = score\n",
    "\n",
    "        return similarities\n",
    "\n",
    "    def get_top_k_similar_documents(self, query_tokens, k):\n",
    "        similarities = self.compute_similarity_batch(query_tokens)\n",
    "\n",
    "        if k >= len(similarities):\n",
    "            top_k_indices = np.argsort(similarities)[::-1]      \n",
    "        else:\n",
    "            top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
    "            top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])[::-1]]\n",
    "        \n",
    "        # need to plus 1 to indices for 1-based indexing\n",
    "        top_k_indices += 1\n",
    "        return top_k_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb9dce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25()\n",
    "\n",
    "for code in code_snippets:\n",
    "    bm25.add_document(code)\n",
    "\n",
    "top_k_results = []\n",
    "for query in queries:\n",
    "    top_k = bm25.get_top_k_similar_documents(query, 10)\n",
    "    top_k_results.append(top_k)\n",
    "\n",
    "output_submission_file(top_k_results, 'results/bm25_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42323e28",
   "metadata": {},
   "source": [
    "# Dense Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a17d32",
   "metadata": {},
   "source": [
    "## Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c2c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:34<00:00,  2.18s/it]\n",
      "100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def tokens_to_ids_batch(batch_tokens, max_len, pad_token_id):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for tokens in batch_tokens:\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # ids = [tokenizer.cls_token_id] + ids + [tokenizer.sep_token_id]\n",
    "\n",
    "        ids = ids[:max_len]\n",
    "        attn_mask = [1] * len(ids)\n",
    "        while len(ids) < max_len:\n",
    "            ids.append(pad_token_id)\n",
    "            attn_mask.append(0)\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        attention_masks.append(attn_mask)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_masks)\n",
    "    }\n",
    "\n",
    "def encode_tokens_list(tokens_list, max_len=256, batch_size=32):\n",
    "    embeddings = []\n",
    "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id \n",
    "    for i in tqdm(range(0, len(tokens_list), batch_size)):\n",
    "        batch_tokens = tokens_list[i:i+batch_size]\n",
    "        encoded_input = tokens_to_ids_batch(batch_tokens, max_len, pad_token_id)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        batch_embeddings = model_output.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.vstack(embeddings) \n",
    "\n",
    "code_snippets = pd.read_csv(out_codes_csv, engine=\"python\")['code_tokens'].apply(ast.literal_eval).tolist()\n",
    "queries = pd.read_csv(out_queries_csv, engine=\"python\")['query_tokens'].apply(ast.literal_eval).tolist()\n",
    "\n",
    "code_embeds = encode_tokens_list(code_snippets, max_len=256)\n",
    "query_embeds = encode_tokens_list(queries, max_len=128)\n",
    "\n",
    "code_embeds = normalize(code_embeds)\n",
    "query_embeds = normalize(query_embeds)\n",
    "\n",
    "similarity_matrix = cosine_similarity(query_embeds, code_embeds)\n",
    "\n",
    "top_k = 10\n",
    "results = []\n",
    "for i in range(len(queries)):\n",
    "    top_indices = similarity_matrix[i].argsort()[-top_k:][::-1]\n",
    "    top_indices += 1  # Convert to 1-based indexing\n",
    "    results.append(top_indices.tolist())\n",
    "\n",
    "output_submission_file(results, 'results/pre_trained_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ac5c7",
   "metadata": {},
   "source": [
    "## Fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b0e0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coconut/114-1/IR/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune a pre-trained model on a data/train_queries.csv which contains code snippets and their corresponding queries.\n",
    "# data only contain code and query\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "class CodeQuerySimilarityModel(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size=768):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "        self.similarity_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, code_input_ids, code_attention_mask, query_input_ids, query_attention_mask):\n",
    "        code_outputs = self.encoder(input_ids=code_input_ids, attention_mask=code_attention_mask)\n",
    "        code_embed = code_outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "        query_outputs = self.encoder(input_ids=query_input_ids, attention_mask=query_attention_mask)\n",
    "        query_embed = query_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        combined = torch.cat([code_embed, query_embed], dim=-1)\n",
    "        similarity = self.similarity_head(combined)\n",
    "        \n",
    "        return similarity.squeeze(), code_embed, query_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732756ee",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a6650df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (500, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1547713.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 1000 (positive: 500, negative: 500)\n",
      "Train samples: 800, Validation samples: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class CodeQueryDataset(Dataset):\n",
    "    def __init__(self, codes, queries, labels, tokenizer, max_code_len=512, max_query_len=128):\n",
    "        self.codes = codes\n",
    "        self.queries = queries\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_code_len = max_code_len\n",
    "        self.max_query_len = max_query_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        code = str(self.codes[idx])\n",
    "        query = str(self.queries[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        code_encoding = self.tokenizer(\n",
    "            code,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_code_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        query_encoding = self.tokenizer(\n",
    "            query,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_query_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'code_input_ids': code_encoding['input_ids'].squeeze(),\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(),\n",
    "            'query_input_ids': query_encoding['input_ids'].squeeze(),\n",
    "            'query_attention_mask': query_encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "train_df = pd.read_csv('data/train_queries.csv')\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "\n",
    "train_df['code'] = train_df['code'].apply(clean_text)\n",
    "train_df['query'] = train_df['query'].apply(clean_text)\n",
    "\n",
    "positive_codes = train_df['code'].tolist()\n",
    "positive_queries = train_df['query'].tolist()\n",
    "positive_labels = [1.0] * len(positive_codes)\n",
    "\n",
    "negative_codes = []\n",
    "negative_queries = []\n",
    "negative_labels = []\n",
    "\n",
    "n_negatives = len(positive_codes)\n",
    "\n",
    "for i in tqdm(range(n_negatives)):\n",
    "    # 隨機選擇不同的 query\n",
    "    neg_idx = random.randint(0, len(positive_queries) - 1)\n",
    "    while neg_idx == i:  # 確保不是同一對\n",
    "        neg_idx = random.randint(0, len(positive_queries) - 1)\n",
    "    \n",
    "    negative_codes.append(positive_codes[i])\n",
    "    negative_queries.append(positive_queries[neg_idx])\n",
    "    negative_labels.append(0.0)\n",
    "\n",
    "all_codes = positive_codes + negative_codes\n",
    "all_queries = positive_queries + negative_queries\n",
    "all_labels = positive_labels + negative_labels\n",
    "\n",
    "indices = list(range(len(all_codes)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "all_codes = [all_codes[i] for i in indices]\n",
    "all_queries = [all_queries[i] for i in indices]\n",
    "all_labels = [all_labels[i] for i in indices]\n",
    "\n",
    "print(f\"Total training samples: {len(all_codes)} (positive: {len(positive_codes)}, negative: {len(negative_codes)})\")\n",
    "\n",
    "split_idx = int(0.8 * len(all_codes))\n",
    "train_codes = all_codes[:split_idx]\n",
    "train_queries = all_queries[:split_idx]\n",
    "train_labels = all_labels[:split_idx]\n",
    "\n",
    "val_codes = all_codes[split_idx:]\n",
    "val_queries = all_queries[split_idx:]\n",
    "val_labels = all_labels[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_codes)}, Validation samples: {len(val_codes)}\")\n",
    "\n",
    "train_dataset = CodeQueryDataset(train_codes, train_queries, train_labels, tokenizer)\n",
    "val_dataset = CodeQueryDataset(val_codes, val_queries, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775787b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練循環\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        code_input_ids = batch['code_input_ids'].to(device)\n",
    "        code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "        query_input_ids = batch['query_input_ids'].to(device)\n",
    "        query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions, code_embeds, query_embeds = model(\n",
    "            code_input_ids, code_attention_mask,\n",
    "            query_input_ids, query_attention_mask\n",
    "        )\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # 反向傳播\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 統計\n",
    "        total_loss += loss.item()\n",
    "        predicted_labels = (predictions > 0.5).float()\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct_predictions/total_predictions:.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            query_input_ids = batch['query_input_ids'].to(device)\n",
    "            query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            predictions, _, _ = model(\n",
    "                code_input_ids, code_attention_mask,\n",
    "                query_input_ids, query_attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecf43a1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb4587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  92%|█████████▏| 92/100 [10:42<00:55,  6.99s/it, loss=0.6153, acc=0.4742]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n\u001b[32m     31\u001b[39m train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, scheduler, criterion, device)\u001b[39m\n\u001b[32m     15\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     17\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m predictions, code_embeds, query_embeds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcode_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_attention_mask\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m loss = criterion(predictions, labels)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 反向傳播\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mCodeQuerySimilarityModel.forward\u001b[39m\u001b[34m(self, code_input_ids, code_attention_mask, query_input_ids, query_attention_mask)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, code_input_ids, code_attention_mask, query_input_ids, query_attention_mask):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     code_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     code_embed = code_outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n\u001b[32m     47\u001b[39m     query_outputs = \u001b[38;5;28mself\u001b[39m.encoder(input_ids=query_input_ids, attention_mask=query_attention_mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:862\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    857\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    858\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    860\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    876\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:606\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    602\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    604\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:513\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    512\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    522\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:440\u001b[39m, in \u001b[36mRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    439\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    450\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/114-1/IR/.venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:363\u001b[39m, in \u001b[36mRobertaSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[32m    361\u001b[39m is_causal = \u001b[38;5;28mself\u001b[39m.is_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len > \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    373\u001b[39m attn_output = attn_output.reshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m.all_head_size)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "batch_size = 8  # 根據GPU記憶體調整\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = CodeQuerySimilarityModel(base_model)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
    "    \n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Saved best model!\")\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd14f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for inference...\n",
      "Encoding code snippets with fine-tuned model...\n",
      "Encoding code snippets with fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 63/63 [00:35<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding queries with fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 32/32 [00:11<00:00,  2.69it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code embeddings shape: (500, 768)\n",
      "Query embeddings shape: (500, 768)\n",
      "Computing similarity matrix...\n",
      "Fine-tuned model results saved to fine_tuned_submission.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: 0.7008\n",
      "Final validation loss: 0.6949\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading best model for inference...\")\n",
    "\n",
    "inference_model = CodeQuerySimilarityModel(base_model)\n",
    "inference_model.to(device)\n",
    "inference_model.load_state_dict(torch.load('best_model.pth'))\n",
    "inference_model.eval()\n",
    "\n",
    "def encode_texts_finetuned(texts, max_len=512, batch_size=16):\n",
    "    embeddings = []\n",
    "    inference_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = inference_model.encoder(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "code_embeddings_ft = encode_texts_finetuned(codes, max_len=512, batch_size=8)\n",
    "\n",
    "query_embeddings_ft = encode_texts_finetuned(queries, max_len=128, batch_size=16)\n",
    "\n",
    "print(f\"Code embeddings shape: {code_embeddings_ft.shape}\")\n",
    "print(f\"Query embeddings shape: {query_embeddings_ft.shape}\")\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "code_embeddings_ft_norm = normalize(code_embeddings_ft, norm='l2')\n",
    "query_embeddings_ft_norm = normalize(query_embeddings_ft, norm='l2')\n",
    "\n",
    "print(\"Computing similarity matrix...\")\n",
    "similarity_matrix_ft = np.dot(query_embeddings_ft_norm, code_embeddings_ft_norm.T)\n",
    "\n",
    "top_k = 10\n",
    "results_ft = []\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    if top_k >= len(codes):\n",
    "        top_indices = np.argsort(similarity_matrix_ft[i])[::-1]\n",
    "    else:\n",
    "        top_indices = np.argpartition(similarity_matrix_ft[i], -top_k)[-top_k:]\n",
    "        top_indices = top_indices[np.argsort(similarity_matrix_ft[i][top_indices])[::-1]]\n",
    "    \n",
    "    results_ft.append(top_indices.tolist())\n",
    "\n",
    "output_submission_file(results_ft, 'fine_tuned_submission.csv')\n",
    "print(\"Fine-tuned model results saved to fine_tuned_submission.csv\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
